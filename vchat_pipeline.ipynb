{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded18ab8-56e6-4dbc-9e78-b5d758e1da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# for chat history (memory), system + user prompts\n",
    "# LLM chain (for tying everything together)\n",
    "# Gemini chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dc22aa-4db4-4540-bf95-60eba1cc7038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API_KEY Loaded: True\n",
      "GOOGLE_API_KEY Loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables (API keys) from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check if API keys loaded\n",
    "print(f\"PINECONE_API_KEY Loaded: {bool(PINECONE_API_KEY)}\")\n",
    "print(f\"GOOGLE_API_KEY Loaded: {bool(GOOGLE_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa51f590-c8c1-4ffe-94c5-36366171e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index and initialized embedding model.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Connect to the existing index\n",
    "pinecone_index = pc.Index(\"vchat\")\n",
    "\n",
    "# Initialize Google Gemini embedding model\n",
    "embed_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"Connected to Pinecone index and initialized embedding model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a98cf6-29ca-46f8-98cb-47a822e49b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt for Gemini 2.0\n",
    "system_prompt_template = \"\"\"\n",
    "you are vchat, an AI assistant for a start up product called villam hub.\n",
    "Answer questions very very briefly and accurately. Use the following information to answer the user's question:\n",
    "\n",
    "{doc_content}\n",
    "\n",
    "Provide very brief accurate and helpful health response based on the provided information and your expertise.\n",
    "\"\"\"\n",
    "# {doc_content} is a placeholder where we'll insert relevant text from our data source.\n",
    "\n",
    "# Function to retrieve top matching chunks from Pinecone\n",
    "def retrieve_relevant_chunks(question):\n",
    "    # Embed the user question\n",
    "    query_vector = embed_model.embed_query(question)\n",
    "    query_vector = [float(x) for x in query_vector]  # wrap it in a floats for pinecone compatibility\n",
    "\n",
    "    # Query Pinecone for top 3 most similar text chunks\n",
    "    search_results = pinecone_index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=2,\n",
    "        include_values=False,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    # Extract 'text' field from metadata of matched results\n",
    "    top_chunks = [match[\"metadata\"].get(\"text\", \"\") for match in search_results.get(\"matches\", [])]\n",
    "        \n",
    "    # Return concatenated result or fallback if nothing found\n",
    "    if not top_chunks:\n",
    "        return \"No relevant information found.\"\n",
    "        \n",
    "    # Clean chunks\n",
    "    clean_chunk = [f\"- {chunk.strip()}\" for chunk in top_chunks]\n",
    "\n",
    "    # Escape curly braces in content to prevent format() issues in the prompt\n",
    "    return \"\\n\".join(clean_chunk).replace(\"{\", \"{{\").replace(\"}\", \"}}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b2194d8-5c68-4561-a5d8-ed4dc801c662",
   "metadata": {},
   "source": [
    "# DEBUG PRINT: goes before extracting text field\n",
    "    print(\"\\nRETRIEVED FROM PINECONE:\")\n",
    "    for i, match in enumerate(search_results[\"matches\"]):\n",
    "        print(f\"Chunk {i+1}: {match['metadata'].get('text', '[NO TEXT]')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900960c6-07f2-4334-881d-6d88bb0eed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to generate response\n",
    "def generate_response(user_question, history=[]):\n",
    "    \"\"\"Generate a response using Pinecone + Gemini 2.0 with optional memory.\"\"\"\n",
    "    \n",
    "    # 1. Retrieve the most relevant chunks from Pinecone\n",
    "    context = retrieve_relevant_chunks(user_question)\n",
    "    print('Retrieved context from pinecone: \\n', context)\n",
    "    \n",
    "    # 2. Format the system prompt using the retrieved content\n",
    "    system_prompt = system_prompt_template.format(doc_content=context)\n",
    "       \n",
    "    # 3. Convert the passed chat history to LangChain format\n",
    "    chat_history = ChatMessageHistory()\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            chat_history.add_user_message(msg[\"content\"])\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            chat_history.add_ai_message(msg[\"content\"])\n",
    "  \n",
    "    # 4. Initialize memory for the chain\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        chat_memory=chat_history,\n",
    "        return_messages=True\n",
    "    )\n",
    "    # 5. Define the full chat prompt\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(system_prompt),   # gives VillamBot its role + retrieved info\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),          # allows past chat to be included\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\")      # inserts user's current question\n",
    "        ]\n",
    "    )\n",
    "    # 6. Load the Gemini 2.0 Flash LLM\n",
    "    chat_model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    # 7. Combine LLM, prompt, and memory into a conversation chain\n",
    "    conversation = LLMChain(\n",
    "        llm=chat_model,\n",
    "        prompt=prompt,\n",
    "        memory=memory,\n",
    "        verbose=True  # helps with debugging/logging\n",
    "    )\n",
    "    \n",
    "    # 8. Ask the question and get the final answer\n",
    "    result = conversation({\"question\": user_question})\n",
    "    print(\"Prompt passed to Gemini:\\n\", system_prompt)\n",
    "    return result.get(\"text\", \"Sorry, I couldn't find an answer.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75cb3cae-9e47-4b18-bb28-b81012222eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context from pinecone: \n",
      " - text: Villam Hub is an urban farming platform focused on helping people grow food, plant trees, and live more sustainably. It combines digital technology with offline experiences.\n",
      "source: VillamHub_Overview\n",
      "- text: The founder of Villam Hub is Wisdom Chibuzor. He is a Product Manager and a farmer who is passionate about sustainable agriculture.\n",
      "source: VillamHub_Overview\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olamide\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "you are vchat, an AI assistant for a start up product called villam hub.\n",
      "Answer questions very very briefly and accurately. Use the following information to answer the user's question:\n",
      "\n",
      "- text: Villam Hub is an urban farming platform focused on helping people grow food, plant trees, and live more sustainably. It combines digital technology with offline experiences.\n",
      "source: VillamHub_Overview\n",
      "- text: The founder of Villam Hub is Wisdom Chibuzor. He is a Product Manager and a farmer who is passionate about sustainable agriculture.\n",
      "source: VillamHub_Overview\n",
      "\n",
      "Provide very brief accurate and helpful health response based on the provided information and your expertise.\n",
      "\n",
      "Human: who is the founder of villam hub?\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt passed to Gemini:\n",
      " \n",
      "you are vchat, an AI assistant for a start up product called villam hub.\n",
      "Answer questions very very briefly and accurately. Use the following information to answer the user's question:\n",
      "\n",
      "- text: Villam Hub is an urban farming platform focused on helping people grow food, plant trees, and live more sustainably. It combines digital technology with offline experiences.\n",
      "source: VillamHub_Overview\n",
      "- text: The founder of Villam Hub is Wisdom Chibuzor. He is a Product Manager and a farmer who is passionate about sustainable agriculture.\n",
      "source: VillamHub_Overview\n",
      "\n",
      "Provide very brief accurate and helpful health response based on the provided information and your expertise.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unfortunately, there is no widely known or easily verifiable information about a company called \"Villam Hub\" or its founder. A general internet search doesn\\'t yield any prominent results for such a company.\\n\\nIt\\'s possible that:\\n\\n*   **It\\'s a very new or small company:** In this case, it might not have a significant online presence yet.\\n*   **The name is slightly different:** There might be a typo in the name.\\n*   **It\\'s a local or niche business:** It might operate in a specific geographic area or industry that isn\\'t well-documented online.\\n*   **The information is not publicly available:** The founder might prefer to keep their identity private.\\n\\nIf you have more context about Villam Hub (e.g., its industry, location, or what it does), that might help in finding more information.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response('who is the founder of villam hub?\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc1359-5bac-4e2d-a3ac-94b49cd84f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
