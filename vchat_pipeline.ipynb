{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ded18ab8-56e6-4dbc-9e78-b5d758e1da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# for chat history (memory), system + user prompts\n",
    "# LLM chain (for tying everything together)\n",
    "# Gemini chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dc22aa-4db4-4540-bf95-60eba1cc7038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API_KEY Loaded: True\n",
      "GOOGLE_API_KEY Loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from .env variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check if API keys loaded\n",
    "print(f\"PINECONE_API_KEY Loaded: {bool(PINECONE_API_KEY)}\")\n",
    "print(f\"GOOGLE_API_KEY Loaded: {bool(GOOGLE_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa51f590-c8c1-4ffe-94c5-36366171e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index and initialized embedding model.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Connect to the existing index\n",
    "pinecone_index = pc.Index(\"vchat\")\n",
    "\n",
    "# Initialize Google Gemini embedding model\n",
    "embed_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"Connected to Pinecone index and initialized embedding model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a98cf6-29ca-46f8-98cb-47a822e49b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt for Gemini 2.0\n",
    "system_prompt_template = \"\"\"\n",
    "you are vchat, an AI assistant for a start up product called villam hub.\n",
    "Answer questions very very briefly and accurately. Use the following information to answer the user's question:\n",
    "\n",
    "{doc_content}\n",
    "\n",
    "Provide very brief accurate and helpful health response based on the provided information and your expertise.\n",
    "\"\"\"\n",
    "# {doc_content} is a placeholder where we'll insert relevant text from our data source.\n",
    "\n",
    "# Function to retrieve top matching chunks from Pinecone\n",
    "def retrieve_relevant_chunks(question):\n",
    "    # Embed the user question\n",
    "    query_vector = embed_model.embed_query(question)\n",
    "    query_vector = [float(x) for x in query_vector]  # wrap it in a floats for pinecone compatibility\n",
    "\n",
    "    # Query Pinecone for top 3 most similar text chunks\n",
    "    search_results = pinecone_index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=2,\n",
    "        include_values=False,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    # Extract 'text' field from metadata of matched results\n",
    "    top_chunks = [match[\"metadata\"].get(\"text\", \"\") for match in search_results.get(\"matches\", [])]\n",
    "        \n",
    "    # Return concatenated result or fallback if nothing found\n",
    "    if not top_chunks:\n",
    "        return \"No relevant information found.\"\n",
    "        \n",
    "    # Clean chunks\n",
    "    clean_chunk = [f\"- {chunk.strip()}\" for chunk in top_chunks]\n",
    "\n",
    "    # Escape curly braces in content to prevent format() issues in the prompt\n",
    "    return \"\\n\".join(clean_chunk).replace(\"{\", \"{{\").replace(\"}\", \"}}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b2194d8-5c68-4561-a5d8-ed4dc801c662",
   "metadata": {},
   "source": [
    "# DEBUG PRINT: goes before extracting text field\n",
    "    print(\"\\nRETRIEVED FROM PINECONE:\")\n",
    "    for i, match in enumerate(search_results[\"matches\"]):\n",
    "        print(f\"Chunk {i+1}: {match['metadata'].get('text', '[NO TEXT]')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900960c6-07f2-4334-881d-6d88bb0eed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to generate response\n",
    "def generate_response(user_question, history=[]):\n",
    "    \"\"\"Generate a response using Pinecone + Gemini 2.0 with optional memory.\"\"\"\n",
    "    \n",
    "    # Retrieve the most relevant chunks from Pinecone\n",
    "    context = retrieve_relevant_chunks(user_question)\n",
    "    print('Retrieved context from pinecone: \\n', context)\n",
    "    \n",
    "    # Format the system prompt using the retrieved content\n",
    "    system_prompt = system_prompt_template.format(doc_content=context)\n",
    "       \n",
    "    # Convert the passed chat history to LangChain format\n",
    "    chat_history = ChatMessageHistory()\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            chat_history.add_user_message(msg[\"content\"])\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            chat_history.add_ai_message(msg[\"content\"])\n",
    "  \n",
    "    # Initialize memory for the chain\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        chat_memory=chat_history,\n",
    "        return_messages=True\n",
    "    )\n",
    "    # Define the full chat prompt\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(system_prompt),   # gives V-Chat its role + retrieved info\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),          # allows past chat to be included\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\")      # inserts user's current question\n",
    "        ]\n",
    "    )\n",
    "    # Load the Gemini 2.0 Flash LLM\n",
    "    chat_model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    # Combine LLM, prompt, and memory into a conversation chain\n",
    "    conversation = LLMChain(\n",
    "        llm=chat_model,\n",
    "        prompt=prompt,\n",
    "        memory=memory,\n",
    "        verbose=True  # helps with debugging/logging\n",
    "    )\n",
    "    \n",
    "    # Ask the question and get the final answer\n",
    "    result = conversation({\"question\": user_question})\n",
    "    print(\"Prompt passed to Gemini:\\n\", system_prompt)\n",
    "    return result.get(\"text\", \"Sorry, I couldn't find an answer.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5785b59a-3181-494f-a917-b46f0efa2cb8",
   "metadata": {},
   "source": [
    "generate_response('who is the founder of villam hub?\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc1359-5bac-4e2d-a3ac-94b49cd84f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
