{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66dc22aa-4db4-4540-bf95-60eba1cc7038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API_KEY Loaded: True\n",
      "GOOGLE_API_KEY Loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (API keys) from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check if API keys loaded\n",
    "print(f\"PINECONE_API_KEY Loaded: {bool(PINECONE_API_KEY)}\")\n",
    "print(f\"GOOGLE_API_KEY Loaded: {bool(GOOGLE_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa51f590-c8c1-4ffe-94c5-36366171e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index and initialized embedding model.\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Connect to the existing index\n",
    "pinecone_index = pc.Index(\"villambot\")\n",
    "\n",
    "# Initialize Google Gemini embedding model\n",
    "embed_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "print(\"Connected to Pinecone index and initialized embedding model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "82a98cf6-29ca-46f8-98cb-47a822e49b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt for Gemini 2.0\n",
    "system_prompt_template = \"\"\"\n",
    "Your name is VillamBot. you are a friendly and reliable assistant for Villam Hub. You are to Answer questions very very briefly and accurately.\n",
    "\n",
    "Use the following information to answer the user's question:\n",
    "\n",
    "{doc_content}\n",
    "\n",
    "Provide very brief accurate and helpful response based on the provided information and your expertise.\n",
    "\n",
    "\"\"\"\n",
    "# {doc_content} is a placeholder where we'll insert relevant text from our data source.\n",
    "\n",
    "# Function to retrieve top matching chunks from Pinecone\n",
    "def retrieve_relevant_chunks(question):\n",
    "    # Embed the user question\n",
    "    query_vector = embed_model.embed_query(question)\n",
    "    query_vector = [float(x) for x in query_vector]  # wrap it in a floats for pinecone compatibility\n",
    "\n",
    "    # Query Pinecone for top 3 most similar text chunks\n",
    "    search_results = pinecone_index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=3,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    # DEBUG PRINT:\n",
    "    print(\"\\nRETRIEVED FROM PINECONE:\")\n",
    "    for i, match in enumerate(search_results[\"matches\"]):\n",
    "        print(f\"Chunk {i+1}: {match['metadata'].get('text', '[NO TEXT]')}\")\n",
    "\n",
    "    # Extract 'text' field from metadata of matched results\n",
    "    top_chunks = []\n",
    "    for match in search_results.get(\"matches\", []):\n",
    "        content = match[\"metadata\"].get(\"text\", \"\")\n",
    "        top_chunks.append(f\"- {content}\")\n",
    "\n",
    "    # Return concatenated result or fallback if nothing found\n",
    "    if not top_chunks:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    # Escape curly braces in content to prevent format() issues in the prompt\n",
    "    return \"\\n\".join(top_chunks).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0683e1c5-52fd-430f-8f9f-796989725fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import streamlit as st \n",
    "\n",
    "# for chat history (memory), system + user prompts\n",
    "# LLM chain (for tying everything together)\n",
    "# Gemini chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "900960c6-07f2-4334-881d-6d88bb0eed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to generate response\n",
    "def generate_response(user_question, history=[]):\n",
    "    \"\"\"Generate a response using Pinecone + Gemini 2.0 with optional memory.\"\"\"\n",
    "    # 1. Retrieve the most relevant chunks from Pinecone\n",
    "    context = retrieve_relevant_chunks(user_question)\n",
    "    \n",
    "    # 2. Format the system prompt using the retrieved content\n",
    "    system_prompt = system_prompt_template.format(doc_content=context)\n",
    "       \n",
    "    # 3. Convert the passed chat history to LangChain format\n",
    "    chat_history = ChatMessageHistory()\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            chat_history.add_user_message(msg[\"content\"])\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            chat_history.add_ai_message(msg[\"content\"])\n",
    "  \n",
    "    # 4. Initialize memory for the chain\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        chat_memory=chat_history,\n",
    "        return_messages=True\n",
    "    )\n",
    "    # 5. Define the full chat prompt\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(system_prompt),   # gives VillamBot its role + retrieved info\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),          # allows past chat to be included\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\")      # inserts user's current question\n",
    "        ]\n",
    "    )\n",
    "    # 6. Load the Gemini 2.0 Flash model\n",
    "    chat_model = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.1,\n",
    "        google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "    )\n",
    "    # 7. Combine LLM, prompt, and memory into a conversation chain\n",
    "    conversation = LLMChain(\n",
    "        llm=chat_model,\n",
    "        prompt=prompt,\n",
    "        memory=memory,\n",
    "        verbose=True  # helps with debugging/logging\n",
    "    )\n",
    "    \n",
    "    # 8. Ask the question and get the final answer\n",
    "    result = conversation({\"question\": user_question})\n",
    "    return result.get(\"text\", \"Sorry, I couldn't find an answer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630cdd0-f482-4eb6-b7a4-7bb2316f9c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
