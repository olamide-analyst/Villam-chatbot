{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66dc22aa-4db4-4540-bf95-60eba1cc7038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API_KEY Loaded: True\n",
      "GOOGLE_API_KEY Loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (API keys) from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from environment variables\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check if API keys loaded\n",
    "print(f\"PINECONE_API_KEY Loaded: {bool(PINECONE_API_KEY)}\")\n",
    "print(f\"GOOGLE_API_KEY Loaded: {bool(GOOGLE_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa51f590-c8c1-4ffe-94c5-36366171e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index and initialized embedding model.\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Connect to the existing index\n",
    "pinecone_index = pc.Index(\"villambot\")\n",
    "\n",
    "# Initialize Google Gemini embedding model\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "print(\"Connected to Pinecone index and initialized embedding model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82a98cf6-29ca-46f8-98cb-47a822e49b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt for Gemini 2.0\n",
    "system_prompt_template = \"\"\"\n",
    "You are VillamBot, a warm, grounded, and reliable assistant for Villam Hub.\n",
    "Your role is to support users on their journey to sustainable living by helping them understand our platform services, farming kits, and how to grow food at home.\n",
    "\n",
    "Use the following information to answer the user's question:\n",
    "\n",
    "{doc_content}\n",
    "\n",
    "Respond clearly, accurately, and concisely based on the provided information and your expertise in sustainable agriculture, eco-friendly practices, and urban farming. \n",
    "\n",
    "\"\"\"\n",
    "# {doc_content} is a placeholder where we'll insert relevant text from our data source.\n",
    "\n",
    "# Function to retrieve top matching chunks from Pinecone\n",
    "def retrieve_relevant_chunks(question):\n",
    "    # Embed the user question\n",
    "    query_vector = embed_model.embed_query(question)\n",
    "    query_vector = [float(x) for x in query_vector]  # Ensure compatibility with Pinecone\n",
    "\n",
    "    # Query Pinecone for top 3 most similar text chunks\n",
    "    search_results = pinecone_index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=3,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # Extract 'text' field from metadata of matched results\n",
    "    top_chunks = []\n",
    "    for match in search_results.get(\"matches\", []):\n",
    "        content = match[\"metadata\"].get(\"text\", \"\")\n",
    "        top_chunks.append(content)\n",
    "\n",
    "    # Return concatenated result or fallback if nothing found\n",
    "    if not top_chunks:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    # Escape curly braces in content to prevent format() issues in the prompt\n",
    "    return \"\\n\".join(top_chunks).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8994b-309a-4b66-9182-2a1c3dc2e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question):\n",
    "    \"\"\"Generate a response using Pinecone retrieval and Gemini 2.0 Flash.\"\"\"\n",
    "\n",
    "    # Create a new asyncio event loop (needed when calling async from sync context)\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "    # 1. Embed the user's question\n",
    "    query_embed = embed_model.embed_query(question)\n",
    "    query_embed = [float(val) for val in query_embed]  # Ensure standard Python floats\n",
    "\n",
    "    # 2. Query Pinecone for relevant document chunks\n",
    "    results = pinecone_index.query(\n",
    "        vector=query_embed,\n",
    "        top_k=3,  # Get top 3 most relevant chunks\n",
    "        include_values=False,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # 3. Extract document contents from metadata (stored under \"text\")\n",
    "    doc_contents = []\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"RETRIEVED DOCUMENTS FOR: '{question}'\")\n",
    "    for i, match in enumerate(results.get('matches', [])):\n",
    "        text = match['metadata'].get('text', '')  # Make sure to match the metadata key you used when upserting\n",
    "        doc_contents.append(text)\n",
    "        print(f\"\\nDOCUMENT {i+1}:\\n{text}\\n\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # 4. Join all retrieved content into one string (or show fallback)\n",
    "    doc_content = \"\\n\".join(doc_contents).replace('{', '{{').replace('}', '}}') \\\n",
    "        if doc_contents else \"No additional information found from my knowledge base.\"\n",
    "\n",
    "    # 5. Format the system prompt with the document content\n",
    "    formatted_prompt = system_prompt_template.format(doc_content=doc_content)\n",
    "\n",
    "    # 6. Rebuild chat history from Streamlit's session state\n",
    "    chat_history = ChatMessageHistory()\n",
    "    for msg in st.session_state.chat_history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            chat_history.add_user_message(msg[\"content\"])\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            chat_history.add_ai_message(msg[\"content\"])\n",
    "\n",
    "    # 7. Initialize LangChain memory with that chat history\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        chat_memory=chat_history,\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    # 8. Create the full prompt template\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(formatted_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 9. Initialize Gemini 2.0 Flash model\n",
    "    chat = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        temperature=0.1,\n",
    "        google_api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "\n",
    "    # 10. Create the LangChain chain (LLM + prompt + memory)\n",
    "    conversation = LLMChain(\n",
    "        llm=chat,\n",
    "        prompt=prompt,\n",
    "        memory=memory,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 11. Run the chain and return the assistantâ€™s response\n",
    "    res = conversation({\"question\": question})\n",
    "    return res.get('text', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66581ed3-9a04-4f7f-b2e5-bde0a45f19e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59626262-4abf-4394-8d3a-95b71d4e3393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de8cfd-cc89-455c-91c5-c7f76c230ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
