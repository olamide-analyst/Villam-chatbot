{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3407f3d1-4de3-400d-a792-50997273b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY Loaded: True\n",
      "PINECONE_API_KEY loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv  # to load API keys from .env file \n",
    "\n",
    "# load .env file content (GOOGLE_API_KEY, PINECONE_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# check if API keys loaded\n",
    "print(f'GOOGLE_API_KEY Loaded: {bool(os.getenv('GOOGLE_API_KEY'))}')\n",
    "print(f'PINECONE_API_KEY loaded: {bool(os.getenv('PINECONE_API_KEY'))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1dfb2af-8bfa-48f9-890d-753b616b8791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Villam Hub – Knowledge Base\n",
      "\n",
      "Overview\n",
      "\n",
      "Name: Villam Hub Founder: Wisdom Chibuzor is the founder of villam hub Description: An all-in-one digital platform and physical hub designed to empower Nigerians and Africans with sustainable, low-power farming solutions, community support, and climate resilien\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# Path to the markdown file (now in your folder)\n",
    "md_path = \"villam_hub_knowledge_base.md\"\n",
    "\n",
    "# Load markdown content\n",
    "loader = UnstructuredMarkdownLoader(md_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Show a preview\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(documents[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee1265c-95ec-4683-9295-c264f99af4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split into 12 chunks\n",
      "Sample content:\n",
      " Villam Hub – Knowledge Base\n",
      "\n",
      "Overview\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# the function to split the text into chuncks \n",
    "def split_docs(documents, chunk_size=300, chunk_overlap=30): \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# split the ddocuments using the function \n",
    "docs = split_docs(documents,chunk_size=300, chunk_overlap=30)\n",
    "\n",
    "# check how many chunks\n",
    "print(f'split into {len(docs)} chunks') \n",
    "print(\"Sample content:\\n\", docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73278338-c835-48a4-bef6-27eba401f5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vector: 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# initialize the goggle embedding model (768 dimensions)\n",
    "embed_model = GoogleGenerativeAIEmbeddings(model= 'models/embedding-001')\n",
    "\n",
    "# to test the model\n",
    "test_text = \"Villam Hub empowers urban farmers.\"\n",
    "embedding = embed_model.embed_query(test_text)\n",
    "\n",
    "print(f\"Length of vector: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c57e55-5fb6-4c26-9df7-a725cc956b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # for regex to extract numbers from error messages\n",
    "\n",
    "# Helper function to extract wait time from API error messages if we get rate limited\n",
    "def parse_retry_wait_time(error):\n",
    "    \"\"\"\n",
    "    Looks inside an error message (usually from an API rate limit error),\n",
    "    and tries to find a specific number of seconds the API suggests we should wait before retrying.\n",
    "    Example:\n",
    "    If the error message says \"Too many requests. Retry in 30 seconds\", this function returns 30.\n",
    "    If no number is found, it returns a default wait time of 20 seconds.\n",
    "    \"\"\"\n",
    "    error_text = str(error) \n",
    "    # Search for a pattern like: \"retry in 30 seconds\"\n",
    "    match = re.search(r\"retry in (\\d+) seconds\", error_text, re.IGNORECASE)\n",
    "    # If found, extract the number and return it as integer\n",
    "    return int(match.group(1)) if match else 20  # Fallback to 20 seconds if no number is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8be22e8-f097-44cf-bf01-2e9dbd388986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time   # used to pause between retries if embedding fails\n",
    "\n",
    "# Function to retry embedding in case of temporary errors like rate limits\n",
    "def embed_batch_with_retry(embed_model, batch_contents, max_attempts=3):\n",
    "    \"\"\"\n",
    "    Tries to embed a batch of text chunks using the provided embedding model.\n",
    "    Retries up to `max_attempts` (3) times if it fails due to errors like rate limiting.\n",
    "    Uses dynamic wait time based on the error message if possible.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Attempt to embed the documents\n",
    "            return embed_model.embed_documents(batch_contents)   \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\") \n",
    "             # If it's the last attempt, raise the error to stop execution\n",
    "            if attempt == max_attempts - 1:\n",
    "                print(\"All attempts failed. Exiting.\")\n",
    "                raise\n",
    "            else:\n",
    "                # Use the helpe rfunction to extract recommended wait time or default to 20s\n",
    "                wait_time = parse_retry_wait_time(e)\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "# Basically, the function sends a batch of text chunks to be embedded, and if it fails, it patiently retries 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905c029e-1090-4f2e-bd26-1cade373ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "# Function to embed all document chunks using concurrent threads for speed\n",
    "def concurrent_embed_documents(embed_model, documents, batch_size=50, max_workers=4):\n",
    "    \"\"\"\n",
    "    Splits the full list of `documents` into batches.\n",
    "    Sends each batch to be embedded in parallel using threads.\n",
    "    Returns two lists: all vector embeddings and the original text chunks.\n",
    "    \"\"\"\n",
    "    all_embeddings = []  # Store all the vector outputs (embedded texts) here\n",
    "    all_contents = []    # Store all the original matching text chunks here\n",
    "    futures = []         # Track background embedding tasks\n",
    "\n",
    "    # Create a pool of up to `max_workers` threads to run batches in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Loop through documents in batches of 50 (`batch_size`)\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]                            # Get the current batch\n",
    "            batch_contents = [doc.page_content for doc in batch]        # Extract just the text\n",
    "\n",
    "            # Submit the batch to be processed in the background\n",
    "            future = executor.submit(embed_batch_with_retry, embed_model, batch_contents)\n",
    "            futures.append((future, batch_contents))\n",
    "\n",
    "        # Show progress bar while each future is being processed\n",
    "        for future, contents in tqdm(futures, total=len(futures), desc=\"Embedding batches\"):\n",
    "            try:\n",
    "                batch_embeddings = future.result()                      # Wait for result from the thread\n",
    "                all_embeddings.extend(batch_embeddings)                 # Add results to the main list\n",
    "                all_contents.extend(contents)                           # Save the original texts too\n",
    "            except Exception as e:\n",
    "                print(f\"Batch failed: {e}\")\n",
    "\n",
    "    return all_embeddings, all_contents\n",
    "\n",
    "# the function Breaks the \"documents\" into 50-piece chunks,Embeds them in parallel (faster)...\n",
    "# as well as Handles errors and retrying & Returns all the vectors and their matching text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8f458b-0367-4cd9-bc79-e498ab595294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for all document chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated 12 embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# HOW to use \n",
    "print(\"Generating embeddings for all document chunks...\")\n",
    "all_embeddings, all_batch_content = concurrent_embed_documents(embed_model, docs)\n",
    "print(f\" Generated {len(all_embeddings)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492f0abb-74e5-4856-b69e-d743a23fc319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone index already exists.\n",
      "Connected to Pinecone index: villambot\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Load API key from environment variable\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "# Define  Pinecone index name\n",
    "index_name = \"villambot\"  \n",
    "\n",
    "# Check if the index already exists\n",
    "existing_indexes = pc.list_indexes()\n",
    "existing_index_names = [index.name for index in existing_indexes.indexes]\n",
    "\n",
    "if index_name not in existing_index_names:\n",
    "    # Create a new index if it doesn't exist\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,                          # Must match vector size from embedding model\n",
    "        metric='cosine',                        # Cosine similarity is standard for RAG\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')  # Free-tier friendly setup\n",
    "    )\n",
    "    print(f\"Created Pinecone index: {index_name}\")\n",
    "    time.sleep(60)  # Allow time for the index to be fully ready\n",
    "else:\n",
    "    print(\"Pinecone index already exists.\")\n",
    "\n",
    "# Connect to the index\n",
    "pinecone_index = pc.Index(index_name)\n",
    "print(f\"Connected to Pinecone index: {index_name}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bee91da1-fb63-441c-91d7-63f7172b3c78",
   "metadata": {},
   "source": [
    "#to delete existing vectors\n",
    "pinecone_index.delete(delete_all=True)\n",
    "print(\"Old vectors deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a4917d-20ba-4a67-9674-55b18dd7c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting Pinecone batched upserts...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting batches: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Pinecone vector storage complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# upload vectors to pinecone(the vector database)\n",
    "\n",
    "BATCH_SIZE = 100  # Number of vectors to upload at once\n",
    "\n",
    "# Embed the document chunks in parallel\n",
    "all_embeddings, all_batch_content = concurrent_embed_documents(embed_model, docs)\n",
    "\n",
    "# Prepare vectors to upsert: an ID (as a string), the embedding vector, metadata (original text)\n",
    "vectors_to_upsert = [\n",
    "    (str(idx), embedding, {\"text\": content})\n",
    "    for idx, (embedding, content) in enumerate(zip(all_embeddings, all_batch_content))\n",
    "]\n",
    "\n",
    "# str(idx): Unique ID for each chunk, using the index position   \n",
    "# embedding: The vector (list of 768 float values) from Google GenAI\n",
    "# {\"text\": content}: Metadata dictionary\n",
    "\n",
    "# Upload vectors to Pinecone in batches \n",
    "def batch_upsert(index, vectors, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Uploads the vectors to Pinecone in small batches.\n",
    "    Retries up to 3 times per batch in case of errors (e.g., rate limiting).\n",
    "    \"\"\"\n",
    "    # Split the full list into smaller batches\n",
    "    batches = [vectors[i:i+batch_size] for i in range(0, len(vectors), batch_size)]\n",
    "\n",
    "    for batch_number, batch in enumerate(tqdm(batches, desc=\"Upserting batches\", total=len(batches))):\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                index.upsert(vectors=batch) # Send the batch to Pinecone\n",
    "                break  # Success, Move to next batch\n",
    "            except Exception as e:    # Handle any errors\n",
    "                print(f\"Batch {batch_number+1}, Attempt {attempt+1} failed: {e}\")\n",
    "                if attempt == 2: # If it's the third failure\n",
    "                    print(f\" Batch {batch_number+1} failed after 3 attempts.\")\n",
    "                    raise e  # Stop the script\n",
    "                else:\n",
    "                    time.sleep(10)  # Wait before retrying\n",
    "\n",
    "# Run the upsert \n",
    "print(\"\\n Starting Pinecone batched upserts...\\n\")\n",
    "batch_upsert(pinecone_index, vectors_to_upsert)\n",
    "print(\"\\n Pinecone vector storage complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25301f7b-2660-4ae5-a519-dd063f6b95e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pinecone returned the correct chunk.\n"
     ]
    }
   ],
   "source": [
    "# Test pass/fail\n",
    "founder_found = any(\"Wisdom Chibuzor\" in match.metadata.get(\"text\", \"\") for match in response.matches)\n",
    "\n",
    "if founder_found:\n",
    "    print(\"✅ Pinecone returned the correct chunk.\")\n",
    "else:\n",
    "    print(\"❌ Pinecone failed to return the correct chunk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1fd10-f691-4471-8513-f2ad9f1f1368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
